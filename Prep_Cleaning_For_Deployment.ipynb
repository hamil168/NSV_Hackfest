{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prep_Cleaning_For_Deployment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hamil168/NSV_Hackfest/blob/master/Prep_Cleaning_For_Deployment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "rWl4Ew7bS1nI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "fea005c1-4064-4931-e0c9-ceb6d4eaa3e4"
      },
      "cell_type": "code",
      "source": [
        "# For a fresh Colab instance, clone fresh:\n",
        "!pip install -q xlrd\n",
        "!git clone https://github.com/hamil168/NSV_Hackfest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NSV_Hackfest'...\n",
            "remote: Counting objects: 94, done.\u001b[K\n",
            "remote: Total 94 (delta 0), reused 0 (delta 0), pack-reused 94\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XskgvhhFS5Yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ea90517-42b1-4726-ce06-76f2bd530ea4"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mNSV_Hackfest\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xreMUi2ATAVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaeedeb8-fea6-4d5f-afd6-5129085f980e"
      },
      "cell_type": "code",
      "source": [
        "cd NSV_Hackfest/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NSV_Hackfest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ht1WjQy1TB5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "cb9213e9-92cf-45a5-acc0-0e5fe46c4958"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "behavior.csv      lda.py              randomize_input.ipynb  train_y.npy\r\n",
            "\u001b[0m\u001b[01;34mData\u001b[0m/             lemmatize.ipynb     README.md              w2v.ipynb\r\n",
            "InputLabel.ipynb  lstm.ipynb          test_X.npy\r\n",
            "input_label.npy   model.h5            test_Y.npy\r\n",
            "input_vol.npy     NSV_Cleaning.ipynb  train_X.npy\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Onq7ufeGU1MD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "82cbaf31-a7da-4a9f-8b22-040812cdec9a"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl (23.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.5MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 13.8MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/80/515ffd88a5b9d35dc6fc29b64727a2ee4322690945e44b81bb8cdb47fecb/boto3-1.7.70-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.11.0,>=1.10.70 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/d2/2412e4cb63931bf1bcd0cde9618c92f51f7bfe68db8dc2120cfabc76a0a8/botocore-1.10.70-py2.py3-none-any.whl (4.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.4MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.70->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.70->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 19.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.7.70 botocore-1.10.70 bz2file-0.98 docutils-0.14 gensim-3.5.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-YbkCmMYNXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18dba1e0-0d5b-46b2-c362-ae75ad82deda"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CjTY1R3ATOSQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JIK24hPa8am",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#nltk.download(stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmLs4P07aSHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build Lemmatizer\n",
        "# stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypEzM5F3anzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exclude_chars = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kwtEvx3DVOfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "\n",
        "### WORD2VEC_MODEL = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "INPUT_LENGTH_LIMIT = 6  # specific to NSV_Hackfest model\n",
        "\n",
        "DEBUGGING = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGPqSPGNVpVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(model.wv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsnpiX46VP9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "25da97a7-e016-4092-e37a-c302f634e27e"
      },
      "cell_type": "code",
      "source": [
        "# Get Line of User Input\n",
        "\n",
        "# get a single line of input\n",
        "user_input = \"\"\n",
        "length_ok = False\n",
        "\n",
        "\n",
        "while not length_ok:\n",
        "  \n",
        "  # get a phrase\n",
        "  user_input_string = input(\"enter input string: \")\n",
        "\n",
        "  # split on spaces\n",
        "  user_input_list = user_input_string.split(' ')\n",
        "\n",
        "  # check against length limit\n",
        "  if len(user_input_list) > INPUT_LENGTH_LIMIT:\n",
        "    print('\\ninput exceeds {} characters'.format(INPUT_LENGTH_LIMIT))\n",
        "  else:\n",
        "    print('\\ninput length OK')\n",
        "    length_ok = True"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter input string: lalala lala la\n",
            "\n",
            "input length OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j6eEtpjKVlC-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"def clean_lemmatize(doc):\n",
        "    ### lemmatize and clean doc\n",
        "    stop_filtered = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_filtered = ''.join(ch for ch in stop_filtered if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_filtered.split())\n",
        "    return normalized\"\"\"\n",
        "\n",
        "def clean_user_input(user_input_string)\n",
        "    #filter for stop words\n",
        "    stop_filtered = [word for word in user_input_string.lower().split(' ') if word not in stop_words]\n",
        "    \n",
        "    #filter for punctuation\n",
        "    punc_filtered = [word for word in punc_filtered if word not in exclude_chars]\n",
        "    \n",
        "    # lemmatize\n",
        "    lemma_filtered = [lemma.lemmatize(word) for word in punc_filtered]\n",
        "    \n",
        "    return lemma_filtered\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o86HXuC6eH8D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_volume(user_input_string, rnn_time_step):\n",
        "  input_list = clean_user_input(user_input_string)\n",
        "  \n",
        "  # w2v volume has 3 components:\n",
        "  # arg1: # of rows; here it is 1, in training it is number of training examples\n",
        "  # arg2: # of words for the rnn timesteps\n",
        "  # arg3: # of elements in the w2v encoding, 300 for the NSV_Hackfest model\n",
        "  w2v = np.zeros(1, rnn_timesteps, W2V_LENGTH)\n",
        "  \n",
        "  # x is a default w2v for a single word; all zeros\n",
        "  x = np.zeros([INPUT_LENGTH_LIMIT, W2V_LENGTH])\n",
        "  \n",
        "  w2v_idx = 0\n",
        "  for word in input_list:\n",
        "    \n",
        "    try:\n",
        "      x = model[word]\n",
        "    except KeyError:\n",
        "      pass\n",
        "    \n",
        "    w2v[w2v_idx] = x\n",
        "    w2v_idx += 1     \n",
        "    \n",
        "  return w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZolzSjFc5lP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This can go to the model\n",
        "\n",
        "input_volume = input_volume(user_input_string, INPUT_LENGTH_LIMIT)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}