{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "### Preparing Model for Deployment\n",
    "- Create data cleaning pipeline for a single user input\n",
    "- Load pre-trained model\n",
    "- Calculate\n",
    "\n",
    "Special Notes:\n",
    "- This model relies on GoogleNews 3000, which is 1.5gig ZIPPED, and must be in a ./model folder relative to this one\n",
    "- requires gensim, nltk\n",
    "- The first draft of this notebook was done in google colab, and there are commented out portions of this notebook left in for posterity should I go back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Colab Specific Cells:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "rWl4Ew7bS1nI",
    "outputId": "fea005c1-4064-4931-e0c9-ceb6d4eaa3e4"
   },
   "outputs": [],
   "source": [
    "# For a fresh Colab instance, clone fresh:\n",
    "#!pip install -q xlrd\n",
    "#!git clone https://github.com/hamil168/NSV_Hackfest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XskgvhhFS5Yf",
    "outputId": "7ea90517-42b1-4726-ce06-76f2bd530ea4"
   },
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xreMUi2ATAVT",
    "outputId": "aaeedeb8-fea6-4d5f-afd6-5129085f980e"
   },
   "outputs": [],
   "source": [
    "#cd NSV_Hackfest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "Ht1WjQy1TB5e",
    "outputId": "cb9213e9-92cf-45a5-acc0-0e5fe46c4958"
   },
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "Onq7ufeGU1MD",
    "outputId": "82cbaf31-a7da-4a9f-8b22-040812cdec9a"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "H-YbkCmMYNXv",
    "outputId": "18dba1e0-0d5b-46b2-c362-ae75ad82deda"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Begin Work***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjTY1R3ATOSQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JIK24hPa8am"
   },
   "outputs": [],
   "source": [
    "# For some reason, colab couldn't download stopwords\n",
    "# so I switched to local development\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmLs4P07aSHf"
   },
   "outputs": [],
   "source": [
    "# Build Lemmatizer\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypEzM5F3anzn"
   },
   "outputs": [],
   "source": [
    "exclude_chars = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CONFIGURATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwtEvx3DVOfk"
   },
   "outputs": [],
   "source": [
    "# This file is huge but it must be present at ./model/ wherever the script is deployed or w2v will not work\n",
    "WORD2VEC_MODEL = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH_LIMIT = 6  # specific to NSV_Hackfest model\n",
    "W2V_LENGTH = 300 # sepecific to this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGPqSPGNVpVz"
   },
   "outputs": [],
   "source": [
    "# type(WORD2VEC_MODEL.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** User Inputs ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "tsnpiX46VP9J",
    "outputId": "25da97a7-e016-4092-e37a-c302f634e27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input length OK\n"
     ]
    }
   ],
   "source": [
    "# Get Line of User Input\n",
    "\n",
    "user_input = \"\"\n",
    "length_ok = False\n",
    "\n",
    "while not length_ok:\n",
    "  \n",
    "  # get a phrase\n",
    "  user_input_string = input(\"enter input string: \")\n",
    "\n",
    "  # split on spaces\n",
    "  user_input_list = user_input_string.split(' ')\n",
    "\n",
    "  # check against length limit\n",
    "  if len(user_input_list) > INPUT_LENGTH_LIMIT:\n",
    "    print('\\ninput exceeds {} characters'.format(INPUT_LENGTH_LIMIT))\n",
    "  else:\n",
    "    print('\\ninput length OK')\n",
    "    length_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6eEtpjKVlC-"
   },
   "outputs": [],
   "source": [
    "def clean_user_input(user_input_string):\n",
    "    \n",
    "    #filter for stop words\n",
    "    stop_filtered = [word for word in user_input_string.lower().split(' ') if word not in stop_words]\n",
    "    \n",
    "    #filter for punctuation\n",
    "    punc_filtered = [word for word in stop_filtered if word not in exclude_chars]\n",
    "    \n",
    "    # lemmatize\n",
    "    lemma_filtered = [lemma.lemmatize(word) for word in punc_filtered]\n",
    "    \n",
    "    return lemma_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing', 'one', 'two', 'three']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_user_input(user_input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prepare input Volume***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o86HXuC6eH8D"
   },
   "outputs": [],
   "source": [
    "def input_volume(user_input_string, rnn_time_steps):\n",
    "  input_list = clean_user_input(user_input_string)\n",
    "  \n",
    "  # w2v volume has 3 components:\n",
    "  # arg1: # of rows; here it is 1, in training it is number of training examples\n",
    "  # arg2: # of words for the rnn_timesteps\n",
    "  # arg3: # of elements in the w2v encoding, 300 for the NSV_Hackfest model\n",
    "  w2v = np.zeros([1, rnn_time_steps, W2V_LENGTH])\n",
    "  \n",
    "  # x is a default w2v for a single word; all zeros\n",
    "  x = np.zeros([W2V_LENGTH])\n",
    "  \n",
    "  w2v_idx = 0\n",
    "  for word in input_list:\n",
    "    \n",
    "    try:\n",
    "      x = WORD2VEC_MODEL[word]\n",
    "    except KeyError:\n",
    "      pass\n",
    "    \n",
    "    w2v[0][w2v_idx] = x\n",
    "    w2v_idx += 1     \n",
    "    \n",
    "  return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZolzSjFc5lP"
   },
   "outputs": [],
   "source": [
    "# This can go to the model\n",
    "\n",
    "input_vol = input_volume(user_input_string, INPUT_LENGTH_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.29101562,  0.02905273,  0.13671875, ...,  0.01275635,\n",
       "         -0.08203125, -0.25976562],\n",
       "        [ 0.0456543 , -0.14550781,  0.15625   , ..., -0.01586914,\n",
       "          0.00671387, -0.00188446],\n",
       "        [ 0.03173828, -0.10644531,  0.00241089, ..., -0.0625    ,\n",
       "         -0.10302734,  0.02929688],\n",
       "        [ 0.04931641, -0.10009766,  0.00665283, ..., -0.02478027,\n",
       "         -0.15917969, -0.02282715],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load Model and Predict***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(input_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0101']\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.8 # same used when we trained the model, doesn't have ot be this high.\n",
    "\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_classification(user_input, model):\n",
    "    \n",
    "    input_vol = input_volume(user_input, INPUT_LENGTH_LIMIT)\n",
    "    \n",
    "    y_pred = model.predict(input_vol)\n",
    "    \n",
    "    yp = []\n",
    "    for label in y_pred:\n",
    "        val = \"\"\n",
    "        for x in label:\n",
    "\n",
    "            val = val + str(int(0 if x < 0.8 else 1))\n",
    "        yp.append(val)\n",
    "    return yp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classification Examples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0101']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_classification(user_input_string, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0110']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_classification(\"jimmy ate his dog\", lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0010']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_classification(\"eating bugs\", lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Validation TODO:***\n",
    "- check order of w2v words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Prep_Cleaning_For_Deployment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
